input {
  # Listens on 514/udp and 514/tcp by default; change that to non-privileged port
  # syslog { port => 51415 }
  gelf { }
  beats {
      type => "filebeat"
      port => "5044"
  }

  kafka {
      type => "kafka"
      bootstrap_servers => "kafka:9092" # kafka from docker compose container_name
      topics => ["log-topic-demo"]
  }
}

# 通过logstash实现filebeat的功能
# input {
#     # 来源beats
#     beats {
#         # 端口
#         port => "5044"
#     }
#     file {
#        type => "server-log"
#        path => "/logs/*.log"
#        start_position => "beginning"
#        codec=>multiline{
#                 // 正则表达式，所有“20”前缀日志, 如果你的日志是以“[2020-06-15”这类前缀则，可以替换成"^["
#                 pattern => "^20"
#                 // 是否对正则规则取反
#                 negate => true
#                 // previous 表示合并到上一行，next 表示合并到下一行
#                 what => "previous"
#         }
#
#     }
# }

# The following filter is a hack! The "de_dot" filter would be better, but it is not pre-installed with logstash by default.
filter {
  ruby {
    code => "event.to_hash.keys.each { |k| event[ k.gsub('.','_') ] = event.remove(k) if k.include?'.' } "
  }
}

output {
  if [type] == "filebeat" {
      elasticsearch {
          hosts => ["http://elasticsearch:9200"]
          index => "%{[@metadata][beat]}-%{[@metadata][version]}-%{+YYYY.MM.dd}" // kibana key pattern
      }
  }

  if [type] == "kafka" {
      elasticsearch {
          hosts => ["http://elasticsearch:9200"]
          index => "kafka-%{+YYYY.MM.dd}"
      }
  }
}